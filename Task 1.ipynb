{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Groq API Tasks: Conversation Management & Information Extraction\n",
        "**Submitted by: Daksh Agarwal**\n",
        "***\n",
        "This notebook demonstrates two core tasks using the Groq API with its OpenAI-compatible SDK, without relying on external frameworks like LangChain.\n",
        "\n",
        "* **Task 1:** Managing and periodically summarizing a conversation history to maintain context over long interactions.\n",
        "* **Task 2:** Extracting structured information (e.g., name, email, location) from unstructured chat messages using a JSON schema and the tool-calling feature.\n",
        "\n",
        "### Instructions for Evaluation\n",
        "To run this notebook, you must configure your own Groq API key.\n",
        "\n",
        "1.  In the Colab menu, click the **key icon (Secrets)** on the left sidebar.\n",
        "2.  Create a new secret with the name `GROQ_API_KEY`.\n",
        "3.  Paste your personal Groq API key into the **Value** field.\n",
        "4.  Ensure the **\"Notebook access\"** toggle is enabled."
      ],
      "metadata": {
        "id": "QGuMVA47LgOU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSojtxnTM7w3",
        "outputId": "664c5c9e-a8fd-46d2-bdf3-7d2023cfbd17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "âœ… Setup Complete: Libraries installed and imported.\n",
            "âœ… Groq API Key loaded successfully.\n",
            "âœ… Groq client initialized.\n",
            "========== demonstrating truncation functionality ==========\n",
            "\n",
            "### Truncation by Number of Turns (last 2 turns) ###\n",
            "\n",
            "Original History Length (messages): 8\n",
            "Truncated History Length (messages): 4\n",
            "--- Conversation History ---\n",
            "  [User]: Great! What are the must-see historical sites in Kyoto?\n",
            "  [Assistant]: In Kyoto, you shouldn't miss Kinkaku-ji (the Golden Pavilion), Fushimi Inari Shrine, and the Arashiyama Bamboo Grove.\n",
            "  [User]: Excellent, thank you for the suggestions.\n",
            "  [Assistant]: You're welcome! Let me know if you need help with hotel or flight bookings.\n",
            "--------------------------\n",
            "\n",
            "### Truncation by Character Length (max 300 chars) ###\n",
            "\n",
            "Original History Length (messages): 8\n",
            "Truncated History Length (messages): 4\n",
            "Total Characters in Truncated History: 288\n",
            "--- Conversation History ---\n",
            "  [User]: Great! What are the must-see historical sites in Kyoto?\n",
            "  [Assistant]: In Kyoto, you shouldn't miss Kinkaku-ji (the Golden Pavilion), Fushimi Inari Shrine, and the Arashiyama Bamboo Grove.\n",
            "  [User]: Excellent, thank you for the suggestions.\n",
            "  [Assistant]: You're welcome! Let me know if you need help with hotel or flight bookings.\n",
            "--------------------------\n",
            "\n",
            "============================================================\n",
            "=========== demonstrating periodic summarization ===========\n",
            "ConversationManager initialized. Summarization will occur after every 3 turns.\n",
            "\n",
            "--- Turn 1 ---\n",
            "[User]: I need to learn about large language models. Where should I start?\n",
            "[Assistant]: Large language models (LLMs) are a rapidly evolving field in the realm of artificial intelligence and natural language processing. Here's a suggested starting point:\n",
            "\n",
            "**1. Understand the basics of language models:**\n",
            " Begin by learning about traditional language models, such as:\n",
            "\t* Language modeling: the task of predicting the next word in a sequence given the context.\n",
            "\t* Neural language models: a type of language model that uses neural networks to predict the next word.\n",
            "\t* Recurrent neural networks (R\n",
            "\n",
            "Current History State:\n",
            "--- Conversation History ---\n",
            "  [User]: I need to learn about large language models. Where should I start?\n",
            "  [Assistant]: Large language models (LLMs) are a rapidly evolving field in the realm of artificial intelligence and natural language processing. Here's a suggested starting point:\n",
            "\n",
            "**1. Understand the basics of language models:**\n",
            " Begin by learning about traditional language models, such as:\n",
            "\t* Language modeling: the task of predicting the next word in a sequence given the context.\n",
            "\t* Neural language models: a type of language model that uses neural networks to predict the next word.\n",
            "\t* Recurrent neural networks (R\n",
            "--------------------------\n",
            "\n",
            "--- Turn 2 ---\n",
            "[User]: What's the difference between fine-tuning and retrieval-augmented generation?\n",
            "[Assistant]: Fine-tuning and retrieval-augmented generation are two techniques used to improve the performance of large language models (LLMs). Here's a brief overview of each and their differences:\n",
            "\n",
            "**Fine-tuning:**\n",
            "\n",
            "Fine-tuning is a technique where a pre-trained LLM is trained on a specific task or dataset to adapt its weights to the new task. This involves:\n",
            "\n",
            "1. Freezing the weights of the pre-trained model (except for the final layer) to retain the learned knowledge.\n",
            "2.\n",
            "\n",
            "Current History State:\n",
            "--- Conversation History ---\n",
            "  [User]: I need to learn about large language models. Where should I start?\n",
            "  [Assistant]: Large language models (LLMs) are a rapidly evolving field in the realm of artificial intelligence and natural language processing. Here's a suggested starting point:\n",
            "\n",
            "**1. Understand the basics of language models:**\n",
            " Begin by learning about traditional language models, such as:\n",
            "\t* Language modeling: the task of predicting the next word in a sequence given the context.\n",
            "\t* Neural language models: a type of language model that uses neural networks to predict the next word.\n",
            "\t* Recurrent neural networks (R\n",
            "  [User]: What's the difference between fine-tuning and retrieval-augmented generation?\n",
            "  [Assistant]: Fine-tuning and retrieval-augmented generation are two techniques used to improve the performance of large language models (LLMs). Here's a brief overview of each and their differences:\n",
            "\n",
            "**Fine-tuning:**\n",
            "\n",
            "Fine-tuning is a technique where a pre-trained LLM is trained on a specific task or dataset to adapt its weights to the new task. This involves:\n",
            "\n",
            "1. Freezing the weights of the pre-trained model (except for the final layer) to retain the learned knowledge.\n",
            "2.\n",
            "--------------------------\n",
            "\n",
            "--- Turn 3 ---\n",
            "[User]: That makes sense. So RAG is better for knowledge-intensive tasks without retraining?\n",
            "[Assistant]: That's a correct observation. Retrieval-Augmented Generation (RAG) is particularly well-suited for knowledge-intensive tasks where the pre-trained model's knowledge may not be sufficient. By retrieving relevant documents or passages from an external knowledge source, RAG can leverage the collective knowledge of the internet and generate more accurate and informative responses.\n",
            "\n",
            "RAG has several advantages over fine-tuning in such scenarios:\n",
            "\n",
            "1. **No retraining required**: RAG can work with pre-trained models without requiring additional fine-t\n",
            "\n",
            ">>> Condition met: Run 3 is a multiple of 3. Triggering summarization...\n",
            ">>> Current history before summarization:\n",
            "--- Conversation History ---\n",
            "  [User]: I need to learn about large language models. Where should I start?\n",
            "  [Assistant]: Large language models (LLMs) are a rapidly evolving field in the realm of artificial intelligence and natural language processing. Here's a suggested starting point:\n",
            "\n",
            "**1. Understand the basics of language models:**\n",
            " Begin by learning about traditional language models, such as:\n",
            "\t* Language modeling: the task of predicting the next word in a sequence given the context.\n",
            "\t* Neural language models: a type of language model that uses neural networks to predict the next word.\n",
            "\t* Recurrent neural networks (R\n",
            "  [User]: What's the difference between fine-tuning and retrieval-augmented generation?\n",
            "  [Assistant]: Fine-tuning and retrieval-augmented generation are two techniques used to improve the performance of large language models (LLMs). Here's a brief overview of each and their differences:\n",
            "\n",
            "**Fine-tuning:**\n",
            "\n",
            "Fine-tuning is a technique where a pre-trained LLM is trained on a specific task or dataset to adapt its weights to the new task. This involves:\n",
            "\n",
            "1. Freezing the weights of the pre-trained model (except for the final layer) to retain the learned knowledge.\n",
            "2.\n",
            "  [User]: That makes sense. So RAG is better for knowledge-intensive tasks without retraining?\n",
            "  [Assistant]: That's a correct observation. Retrieval-Augmented Generation (RAG) is particularly well-suited for knowledge-intensive tasks where the pre-trained model's knowledge may not be sufficient. By retrieving relevant documents or passages from an external knowledge source, RAG can leverage the collective knowledge of the internet and generate more accurate and informative responses.\n",
            "\n",
            "RAG has several advantages over fine-tuning in such scenarios:\n",
            "\n",
            "1. **No retraining required**: RAG can work with pre-trained models without requiring additional fine-t\n",
            "--------------------------\n",
            "\n",
            ">>> Generated Summary:\n",
            "Here's a summary of the conversation:\n",
            "\n",
            "A user requested guidance on learning about large language models (LLMs) and their applications. An assistant suggested starting with the basics of language models, including traditional language modeling, neural language models, and recurrent neural networks. The user then inquired about the differences between fine-tuning and retrieval-augmented generation (RAG). The assistant explained that fine-tuning involves training a pre-trained LLM on a specific task, while RAG leverages an external knowledge source to generate responses by retrieving relevant documents or passages. In knowledge-intensive tasks where retraining is not feasible, RAG is a suitable alternative that can leverage collective knowledge without requiring additional fine-tuning.\n",
            "\n",
            ">>> History has been replaced with the summary.\n",
            "--- Conversation History ---\n",
            "  [System]: This is a summary of the previous conversation: Here's a summary of the conversation:\n",
            "\n",
            "A user requested guidance on learning about large language models (LLMs) and their applications. An assistant suggested starting with the basics of language models, including traditional language modeling, neural language models, and recurrent neural networks. The user then inquired about the differences between fine-tuning and retrieval-augmented generation (RAG). The assistant explained that fine-tuning involves training a pre-trained LLM on a specific task, while RAG leverages an external knowledge source to generate responses by retrieving relevant documents or passages. In knowledge-intensive tasks where retraining is not feasible, RAG is a suitable alternative that can leverage collective knowledge without requiring additional fine-tuning.\n",
            "--------------------------\n",
            "\n",
            "Current History State:\n",
            "--- Conversation History ---\n",
            "  [System]: This is a summary of the previous conversation: Here's a summary of the conversation:\n",
            "\n",
            "A user requested guidance on learning about large language models (LLMs) and their applications. An assistant suggested starting with the basics of language models, including traditional language modeling, neural language models, and recurrent neural networks. The user then inquired about the differences between fine-tuning and retrieval-augmented generation (RAG). The assistant explained that fine-tuning involves training a pre-trained LLM on a specific task, while RAG leverages an external knowledge source to generate responses by retrieving relevant documents or passages. In knowledge-intensive tasks where retraining is not feasible, RAG is a suitable alternative that can leverage collective knowledge without requiring additional fine-tuning.\n",
            "--------------------------\n",
            "\n",
            "--- Turn 4 ---\n",
            "[User]: Okay, let's switch topics. I want to build a simple Python web app.\n",
            "[Assistant]: Building a simple Python web app can be a great way to get started with web development. Here's a high-level overview of the steps you can follow:\n",
            "\n",
            "1. **Choose a framework**: There are several Python web frameworks to choose from, including Flask, Django, Pyramid, and FastAPI. For a simple app, Flask is a good choice.\n",
            "2. **Set up your development environment**: You'll need to install Python, Flask, and a code editor or IDE. You can use pip to\n",
            "\n",
            "Current History State:\n",
            "--- Conversation History ---\n",
            "  [System]: This is a summary of the previous conversation: Here's a summary of the conversation:\n",
            "\n",
            "A user requested guidance on learning about large language models (LLMs) and their applications. An assistant suggested starting with the basics of language models, including traditional language modeling, neural language models, and recurrent neural networks. The user then inquired about the differences between fine-tuning and retrieval-augmented generation (RAG). The assistant explained that fine-tuning involves training a pre-trained LLM on a specific task, while RAG leverages an external knowledge source to generate responses by retrieving relevant documents or passages. In knowledge-intensive tasks where retraining is not feasible, RAG is a suitable alternative that can leverage collective knowledge without requiring additional fine-tuning.\n",
            "  [User]: Okay, let's switch topics. I want to build a simple Python web app.\n",
            "  [Assistant]: Building a simple Python web app can be a great way to get started with web development. Here's a high-level overview of the steps you can follow:\n",
            "\n",
            "1. **Choose a framework**: There are several Python web frameworks to choose from, including Flask, Django, Pyramid, and FastAPI. For a simple app, Flask is a good choice.\n",
            "2. **Set up your development environment**: You'll need to install Python, Flask, and a code editor or IDE. You can use pip to\n",
            "--------------------------\n",
            "\n",
            "--- Turn 5 ---\n",
            "[User]: Which framework is easier for a beginner, Flask or Django?\n",
            "[Assistant]: Between Flask and Django, I'd recommend Flask as a better choice for a beginner. Here's why:\n",
            "\n",
            "1. **Simpler architecture**: Flask has a more minimalist architecture, which means it's easier to understand and work with. Django, on the other hand, has a more complex architecture with a lot of built-in features and abstractions.\n",
            "2. **Fewer dependencies**: Flask has fewer dependencies than Django, which makes it easier to set up and manage. Django, by contrast, has\n",
            "\n",
            "Current History State:\n",
            "--- Conversation History ---\n",
            "  [System]: This is a summary of the previous conversation: Here's a summary of the conversation:\n",
            "\n",
            "A user requested guidance on learning about large language models (LLMs) and their applications. An assistant suggested starting with the basics of language models, including traditional language modeling, neural language models, and recurrent neural networks. The user then inquired about the differences between fine-tuning and retrieval-augmented generation (RAG). The assistant explained that fine-tuning involves training a pre-trained LLM on a specific task, while RAG leverages an external knowledge source to generate responses by retrieving relevant documents or passages. In knowledge-intensive tasks where retraining is not feasible, RAG is a suitable alternative that can leverage collective knowledge without requiring additional fine-tuning.\n",
            "  [User]: Okay, let's switch topics. I want to build a simple Python web app.\n",
            "  [Assistant]: Building a simple Python web app can be a great way to get started with web development. Here's a high-level overview of the steps you can follow:\n",
            "\n",
            "1. **Choose a framework**: There are several Python web frameworks to choose from, including Flask, Django, Pyramid, and FastAPI. For a simple app, Flask is a good choice.\n",
            "2. **Set up your development environment**: You'll need to install Python, Flask, and a code editor or IDE. You can use pip to\n",
            "  [User]: Which framework is easier for a beginner, Flask or Django?\n",
            "  [Assistant]: Between Flask and Django, I'd recommend Flask as a better choice for a beginner. Here's why:\n",
            "\n",
            "1. **Simpler architecture**: Flask has a more minimalist architecture, which means it's easier to understand and work with. Django, on the other hand, has a more complex architecture with a lot of built-in features and abstractions.\n",
            "2. **Fewer dependencies**: Flask has fewer dependencies than Django, which makes it easier to set up and manage. Django, by contrast, has\n",
            "--------------------------\n",
            "\n",
            "--- Turn 6 ---\n",
            "[User]: Thanks. I'll start with Flask. Can you give me a 'Hello, World!' example?\n",
            "[Assistant]: Here's a simple \"Hello, World!\" example using Flask:\n",
            "\n",
            "**app.py**\n",
            "```python\n",
            "from flask import Flask\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "@app.route('/')\n",
            "def hello_world():\n",
            "    return 'Hello, World!'\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(debug=True)\n",
            "```\n",
            "Let me explain what's going on:\n",
            "\n",
            "1. We create a new Flask app instance and assign it to the `app` variable.\n",
            "2. We define a route for the root URL (`\n",
            "\n",
            ">>> Condition met: Run 6 is a multiple of 3. Triggering summarization...\n",
            ">>> Current history before summarization:\n",
            "--- Conversation History ---\n",
            "  [System]: This is a summary of the previous conversation: Here's a summary of the conversation:\n",
            "\n",
            "A user requested guidance on learning about large language models (LLMs) and their applications. An assistant suggested starting with the basics of language models, including traditional language modeling, neural language models, and recurrent neural networks. The user then inquired about the differences between fine-tuning and retrieval-augmented generation (RAG). The assistant explained that fine-tuning involves training a pre-trained LLM on a specific task, while RAG leverages an external knowledge source to generate responses by retrieving relevant documents or passages. In knowledge-intensive tasks where retraining is not feasible, RAG is a suitable alternative that can leverage collective knowledge without requiring additional fine-tuning.\n",
            "  [User]: Okay, let's switch topics. I want to build a simple Python web app.\n",
            "  [Assistant]: Building a simple Python web app can be a great way to get started with web development. Here's a high-level overview of the steps you can follow:\n",
            "\n",
            "1. **Choose a framework**: There are several Python web frameworks to choose from, including Flask, Django, Pyramid, and FastAPI. For a simple app, Flask is a good choice.\n",
            "2. **Set up your development environment**: You'll need to install Python, Flask, and a code editor or IDE. You can use pip to\n",
            "  [User]: Which framework is easier for a beginner, Flask or Django?\n",
            "  [Assistant]: Between Flask and Django, I'd recommend Flask as a better choice for a beginner. Here's why:\n",
            "\n",
            "1. **Simpler architecture**: Flask has a more minimalist architecture, which means it's easier to understand and work with. Django, on the other hand, has a more complex architecture with a lot of built-in features and abstractions.\n",
            "2. **Fewer dependencies**: Flask has fewer dependencies than Django, which makes it easier to set up and manage. Django, by contrast, has\n",
            "  [User]: Thanks. I'll start with Flask. Can you give me a 'Hello, World!' example?\n",
            "  [Assistant]: Here's a simple \"Hello, World!\" example using Flask:\n",
            "\n",
            "**app.py**\n",
            "```python\n",
            "from flask import Flask\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "@app.route('/')\n",
            "def hello_world():\n",
            "    return 'Hello, World!'\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(debug=True)\n",
            "```\n",
            "Let me explain what's going on:\n",
            "\n",
            "1. We create a new Flask app instance and assign it to the `app` variable.\n",
            "2. We define a route for the root URL (`\n",
            "--------------------------\n",
            "\n",
            ">>> Generated Summary:\n",
            "Here's a summary of the conversation: \n",
            "\n",
            "A user initially inquired about learning large language models (LLMs) and their applications. The assistant provided guidance on starting with the basics, including traditional language modeling and neural networks, and explained the differences between fine-tuning and retrieval-augmented generation (RAG). The conversation then shifted to building a simple Python web app. The assistant recommended Flask for a beginner due to its simpler architecture and fewer dependencies compared to Django. A \"Hello, World!\" example using Flask was provided to get the user started, involving a basic route and function to return the string \"Hello, World!\"\n",
            "\n",
            ">>> History has been replaced with the summary.\n",
            "--- Conversation History ---\n",
            "  [System]: This is a summary of the previous conversation: Here's a summary of the conversation: \n",
            "\n",
            "A user initially inquired about learning large language models (LLMs) and their applications. The assistant provided guidance on starting with the basics, including traditional language modeling and neural networks, and explained the differences between fine-tuning and retrieval-augmented generation (RAG). The conversation then shifted to building a simple Python web app. The assistant recommended Flask for a beginner due to its simpler architecture and fewer dependencies compared to Django. A \"Hello, World!\" example using Flask was provided to get the user started, involving a basic route and function to return the string \"Hello, World!\"\n",
            "--------------------------\n",
            "\n",
            "Current History State:\n",
            "--- Conversation History ---\n",
            "  [System]: This is a summary of the previous conversation: Here's a summary of the conversation: \n",
            "\n",
            "A user initially inquired about learning large language models (LLMs) and their applications. The assistant provided guidance on starting with the basics, including traditional language modeling and neural networks, and explained the differences between fine-tuning and retrieval-augmented generation (RAG). The conversation then shifted to building a simple Python web app. The assistant recommended Flask for a beginner due to its simpler architecture and fewer dependencies compared to Django. A \"Hello, World!\" example using Flask was provided to get the user started, involving a basic route and function to return the string \"Hello, World!\"\n",
            "--------------------------\n",
            "\n",
            "--- Turn 7 ---\n",
            "[User]: Perfect! Final question for now: how do I deploy it easily?\n",
            "[Assistant]: Deploying a simple Flask web app can be done in various ways. Here are a few options:\n",
            "\n",
            "1. **Heroku**: Heroku is a popular platform for deploying web applications. You can create a Heroku account, install the Heroku CLI, and deploy your Flask app using `git`. Heroku provides a free tier, making it a great option for small projects.\n",
            "\n",
            "2. **AWS Elastic Beanstalk**: Elastic Beanstalk is a service provided by AWS that allows you to deploy web applications\n",
            "\n",
            "Current History State:\n",
            "--- Conversation History ---\n",
            "  [System]: This is a summary of the previous conversation: Here's a summary of the conversation: \n",
            "\n",
            "A user initially inquired about learning large language models (LLMs) and their applications. The assistant provided guidance on starting with the basics, including traditional language modeling and neural networks, and explained the differences between fine-tuning and retrieval-augmented generation (RAG). The conversation then shifted to building a simple Python web app. The assistant recommended Flask for a beginner due to its simpler architecture and fewer dependencies compared to Django. A \"Hello, World!\" example using Flask was provided to get the user started, involving a basic route and function to return the string \"Hello, World!\"\n",
            "  [User]: Perfect! Final question for now: how do I deploy it easily?\n",
            "  [Assistant]: Deploying a simple Flask web app can be done in various ways. Here are a few options:\n",
            "\n",
            "1. **Heroku**: Heroku is a popular platform for deploying web applications. You can create a Heroku account, install the Heroku CLI, and deploy your Flask app using `git`. Heroku provides a free tier, making it a great option for small projects.\n",
            "\n",
            "2. **AWS Elastic Beanstalk**: Elastic Beanstalk is a service provided by AWS that allows you to deploy web applications\n",
            "--------------------------\n",
            "\n",
            "============================================================\n",
            "âœ… Demonstration Finished.\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "# @title 1. Setup and Dependencies\n",
        "# Install the necessary Python library for Groq\n",
        "!pip install groq\n",
        "\n",
        "import os\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "import json\n",
        "\n",
        "print(\"âœ… Setup Complete: Libraries installed and imported.\")\n",
        "\n",
        "# --- API Key Configuration ---\n",
        "# Securely fetch the API key from Colab's secret manager\n",
        "try:\n",
        "    groq_api_key = userdata.get('GROQ_API_KEY')\n",
        "    os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
        "    print(\"âœ… Groq API Key loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"ðŸš¨ Error loading Groq API Key.\")\n",
        "    print(\"Please ensure you have set the 'GROQ_API_KEY' in Colab's Secrets Manager.\")\n",
        "    # Fallback for demonstration if key isn't set, but API calls will fail.\n",
        "    groq_api_key = \"YOUR_GROQ_API_KEY\"\n",
        "\n",
        "\n",
        "# --- Initialize Groq Client ---\n",
        "try:\n",
        "    client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "    print(\"âœ… Groq client initialized.\")\n",
        "except Exception as e:\n",
        "    client = None\n",
        "    print(f\"ðŸš¨ Failed to initialize Groq client: {e}\")\n",
        "\n",
        "# %%\n",
        "# @title 2. Helper Functions for History Management\n",
        "\n",
        "def format_history_for_display(history):\n",
        "    \"\"\"Helper to pretty-print the conversation history.\"\"\"\n",
        "    formatted_string = \"--- Conversation History ---\\n\"\n",
        "    if not history:\n",
        "        return formatted_string + \"  [Empty]\\n\"\n",
        "    for msg in history:\n",
        "        formatted_string += f\"  [{msg['role'].capitalize()}]: {msg['content']}\\n\"\n",
        "    formatted_string += \"--------------------------\"\n",
        "    return formatted_string\n",
        "\n",
        "def get_assistant_response(messages):\n",
        "    \"\"\"\n",
        "    Gets a response from the Groq API based on the current conversation.\n",
        "    This simulates the assistant's turn in a real chat application.\n",
        "    \"\"\"\n",
        "    if not client:\n",
        "        return \"Groq client not initialized. Cannot get response.\"\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=messages,\n",
        "            # âœ… CORRECTED MODEL NAME\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            temperature=0.7,\n",
        "            max_tokens=100,\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Error communicating with Groq API: {e}\"\n",
        "\n",
        "\n",
        "# --- Truncation Functions ---\n",
        "\n",
        "def truncate_by_turns(history, n_turns):\n",
        "    \"\"\"\n",
        "    Limits the history to the last n conversation turns.\n",
        "    A \"turn\" consists of one user message and one assistant message.\n",
        "    \"\"\"\n",
        "    # Each turn has 2 messages (user + assistant)\n",
        "    max_messages = n_turns * 2\n",
        "    return history[-max_messages:]\n",
        "\n",
        "def truncate_by_length(history, max_chars):\n",
        "    \"\"\"\n",
        "    Limits the history by a maximum character count, starting from the most recent message.\n",
        "    \"\"\"\n",
        "    truncated_history = []\n",
        "    current_chars = 0\n",
        "    # Iterate backwards from the most recent message\n",
        "    for message in reversed(history):\n",
        "        message_chars = len(message.get('content', ''))\n",
        "        if current_chars + message_chars <= max_chars:\n",
        "            # Prepend the message to maintain chronological order\n",
        "            truncated_history.insert(0, message)\n",
        "            current_chars += message_chars\n",
        "        else:\n",
        "            # Stop when the next message would exceed the limit\n",
        "            break\n",
        "    return truncated_history\n",
        "\n",
        "\n",
        "# --- Summarization Function ---\n",
        "\n",
        "def summarize_conversation(history):\n",
        "    \"\"\"\n",
        "    Uses the Groq API to summarize the conversation history.\n",
        "    \"\"\"\n",
        "    if not client:\n",
        "        return \"Groq client not initialized. Cannot summarize.\"\n",
        "\n",
        "    # Convert the history to a simple string format for the prompt\n",
        "    conversation_text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in history])\n",
        "\n",
        "    # Create a new set of messages for the summarization request\n",
        "    summarization_prompt = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert at summarizing conversations. Your task is to create a concise, neutral summary of the following chat history. Capture the main topics, key questions, and any conclusions reached. The summary should be a single, dense paragraph.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": conversation_text\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=summarization_prompt,\n",
        "            # âœ… CORRECTED MODEL NAME\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "        )\n",
        "        summary = chat_completion.choices[0].message.content\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error during summarization: {e}\"\n",
        "\n",
        "\n",
        "# %%\n",
        "# @title 3. Demonstration of Truncation Functionality\n",
        "\n",
        "# --- Sample Conversation History ---\n",
        "sample_history = [\n",
        "    {'role': 'user', 'content': 'Hi, I want to plan a trip to Japan.'},\n",
        "    {'role': 'assistant', 'content': 'Of course! When are you thinking of going and what are your interests?'},\n",
        "    {'role': 'user', 'content': 'I was thinking next April to see the cherry blossoms. I love food and history.'},\n",
        "    {'role': 'assistant', 'content': 'April is a perfect time for cherry blossoms! For food and history, I recommend a route covering Tokyo, Kyoto, and Osaka.'},\n",
        "    {'role': 'user', 'content': 'Great! What are the must-see historical sites in Kyoto?'},\n",
        "    {'role': 'assistant', 'content': 'In Kyoto, you shouldn\\'t miss Kinkaku-ji (the Golden Pavilion), Fushimi Inari Shrine, and the Arashiyama Bamboo Grove.'},\n",
        "    {'role': 'user', 'content': 'Excellent, thank you for the suggestions.'},\n",
        "    {'role': 'assistant', 'content': 'You\\'re welcome! Let me know if you need help with hotel or flight bookings.'}\n",
        "]\n",
        "\n",
        "print(\" demonstrating truncation functionality \".center(60, \"=\"))\n",
        "\n",
        "# --- a. Limit by Number of Conversation Turns ---\n",
        "print(\"\\n### Truncation by Number of Turns (last 2 turns) ###\\n\")\n",
        "truncated_by_turn = truncate_by_turns(sample_history, n_turns=2)\n",
        "print(\"Original History Length (messages):\", len(sample_history))\n",
        "print(\"Truncated History Length (messages):\", len(truncated_by_turn))\n",
        "print(format_history_for_display(truncated_by_turn))\n",
        "\n",
        "# --- b. Limit by Character/Word Length ---\n",
        "print(\"\\n### Truncation by Character Length (max 300 chars) ###\\n\")\n",
        "truncated_by_len = truncate_by_length(sample_history, max_chars=300)\n",
        "total_chars = sum(len(m['content']) for m in truncated_by_len)\n",
        "print(\"Original History Length (messages):\", len(sample_history))\n",
        "print(\"Truncated History Length (messages):\", len(truncated_by_len))\n",
        "print(f\"Total Characters in Truncated History: {total_chars}\")\n",
        "print(format_history_for_display(truncated_by_len))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# %%\n",
        "# @title 4. Conversation Manager with Periodic Summarization\n",
        "\n",
        "class ConversationManager:\n",
        "    \"\"\"\n",
        "    Manages the conversation history, including periodic summarization.\n",
        "    \"\"\"\n",
        "    def __init__(self, k_runs_for_summarization=3):\n",
        "        \"\"\"\n",
        "        Initializes the manager.\n",
        "        Args:\n",
        "            k_runs_for_summarization (int): Summarize after every k-th run.\n",
        "        \"\"\"\n",
        "        self.history = []\n",
        "        self.run_counter = 0\n",
        "        self.k = k_runs_for_summarization\n",
        "        print(f\"ConversationManager initialized. Summarization will occur after every {self.k} turns.\")\n",
        "\n",
        "    def add_turn(self, user_input):\n",
        "        \"\"\"\n",
        "        Handles a single turn of the conversation.\n",
        "        1. Adds user message.\n",
        "        2. Gets and adds assistant message.\n",
        "        3. Increments counter.\n",
        "        4. Checks if it's time to summarize.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Turn {self.run_counter + 1} ---\")\n",
        "\n",
        "        # 1. Add user message\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # 2. Get and add assistant's response\n",
        "        # We pass the current history so the assistant has context\n",
        "        assistant_response = get_assistant_response(self.history)\n",
        "        self.history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "        print(f\"[User]: {user_input}\")\n",
        "        print(f\"[Assistant]: {assistant_response}\")\n",
        "\n",
        "        # 3. Increment the run counter\n",
        "        self.run_counter += 1\n",
        "\n",
        "        # 4. Check for periodic summarization\n",
        "        if self.run_counter % self.k == 0:\n",
        "            print(f\"\\n>>> Condition met: Run {self.run_counter} is a multiple of {self.k}. Triggering summarization...\")\n",
        "            self.summarize_and_replace()\n",
        "\n",
        "    def summarize_and_replace(self):\n",
        "        \"\"\"\n",
        "        Summarizes the current history and replaces it with the summary.\n",
        "        \"\"\"\n",
        "        print(\">>> Current history before summarization:\")\n",
        "        print(format_history_for_display(self.history))\n",
        "\n",
        "        # Get the summary\n",
        "        summary = summarize_conversation(self.history)\n",
        "\n",
        "        print(\"\\n>>> Generated Summary:\")\n",
        "        print(summary)\n",
        "\n",
        "        # Replace the entire history with a single system message containing the summary\n",
        "        self.history = [\n",
        "            {\"role\": \"system\", \"content\": f\"This is a summary of the previous conversation: {summary}\"}\n",
        "        ]\n",
        "\n",
        "        print(\"\\n>>> History has been replaced with the summary.\")\n",
        "        print(format_history_for_display(self.history))\n",
        "\n",
        "# %%\n",
        "# @title 5. Demonstration of Periodic Summarization in Action (k=3)\n",
        "\n",
        "print(\" demonstrating periodic summarization \".center(60, \"=\"))\n",
        "\n",
        "# --- Sample Conversation Flow ---\n",
        "conversation_flow = [\n",
        "    \"I need to learn about large language models. Where should I start?\",\n",
        "    \"What's the difference between fine-tuning and retrieval-augmented generation?\",\n",
        "    \"That makes sense. So RAG is better for knowledge-intensive tasks without retraining?\",\n",
        "    \"Okay, let's switch topics. I want to build a simple Python web app.\",\n",
        "    \"Which framework is easier for a beginner, Flask or Django?\",\n",
        "    \"Thanks. I'll start with Flask. Can you give me a 'Hello, World!' example?\",\n",
        "    \"Perfect! Final question for now: how do I deploy it easily?\",\n",
        "]\n",
        "\n",
        "# Initialize the manager to summarize after every 3rd run\n",
        "manager = ConversationManager(k_runs_for_summarization=3)\n",
        "\n",
        "# Run the conversation flow\n",
        "for user_prompt in conversation_flow:\n",
        "    # Add a guard to stop if the client wasn't initialized\n",
        "    if not client:\n",
        "        print(\"\\nðŸš¨ Halting demonstration because Groq client is not initialized.\")\n",
        "        break\n",
        "    manager.add_turn(user_prompt)\n",
        "    print(\"\\nCurrent History State:\")\n",
        "    print(format_history_for_display(manager.history))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Demonstration Finished.\")"
      ]
    }
  ]
}