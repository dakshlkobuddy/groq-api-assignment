{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Groq API Tasks: Conversation Management & Information Extraction\n",
        "**Submitted by: Daksh Agarwal**\n",
        "***\n",
        "This notebook demonstrates two core tasks using the Groq API with its OpenAI-compatible SDK, without relying on external frameworks like LangChain.\n",
        "\n",
        "* **Task 1:** Managing and periodically summarizing a conversation history to maintain context over long interactions.\n",
        "* **Task 2:** Extracting structured information (e.g., name, email, location) from unstructured chat messages using a JSON schema and the tool-calling feature.\n",
        "\n",
        "### Instructions for Evaluation\n",
        "To run this notebook, you must configure your own Groq API key.\n",
        "\n",
        "1.  In the Colab menu, click the **key icon (Secrets)** on the left sidebar.\n",
        "2.  Create a new secret with the name `GROQ_API_KEY`.\n",
        "3.  Paste your personal Groq API key into the **Value** field.\n",
        "4.  Ensure the **\"Notebook access\"** toggle is enabled."
      ],
      "metadata": {
        "id": "4PA6OlVxLveP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyQL1upViJ_t",
        "outputId": "fbc93adc-e029-47b6-a6df-4cc61b56aea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "‚úÖ Setup Complete: Libraries installed and imported.\n",
            "‚úÖ Groq API Key loaded successfully.\n",
            "‚úÖ Groq client initialized. Using model: llama-3.1-8b-instant\n",
            "======================== TASK 1 DEMONSTRATION ========================\n",
            "ConversationManager initialized. Summarization will occur after every 3 turns.\n",
            "\n",
            "--- Turn 1 ---\n",
            "[User]: I need to learn about large language models. Where should I start?\n",
            "[Assistant]: Large language models (LLMs) are a rapidly evolving field in natural language processing (NLP), and there's a lot to learn. Here's a step-by-step guide to help you get started:\n",
            "\n",
            "**1. Understand the basics of NLP**:\n",
            "Begin by learning about the fundamentals of NLP, including:\n",
            "\t* Tokenization\n",
            "\t* Part-of-speech tagging\n",
            "\t* Named entity recognition\n",
            "\t* Sentiment analysis\n",
            "\t* Text classification\n",
            "\n",
            "You can find many online resources\n",
            "\n",
            "Current History State: --- Conversation History ---\n",
            "  [User]: I need to learn about large language models. Where should I start?\n",
            "  [Assistant]: Large language models (LLMs) are a rapidly evolving field in natural language processing (NLP), and there's a lot to learn. Here's a step-by-step guide to help you get started:\n",
            "\n",
            "**1. Understand the basics of NLP**:\n",
            "Begin by learning about the fundamentals of NLP, including:\n",
            "\t* Tokenization\n",
            "\t* Part-of-speech tagging\n",
            "\t* Named entity recognition\n",
            "\t* Sentiment analysis\n",
            "\t* Text classification\n",
            "\n",
            "You can find many online resources\n",
            "--------------------------\n",
            "\n",
            "--- Turn 2 ---\n",
            "[User]: What's the difference between fine-tuning and retrieval-augmented generation?\n",
            "[Assistant]: Fine-tuning and retrieval-augmented generation are two popular techniques used in large language models (LLMs) to improve their performance on specific tasks or domains. Here's a brief overview of each technique:\n",
            "\n",
            "**Fine-tuning:**\n",
            "\n",
            "Fine-tuning involves training a pre-trained LLM on a specific dataset or task to adapt it to the target domain. This typically involves:\n",
            "\n",
            "1. Loading a pre-trained LLM (e.g., BERT, RoBERTa, or T5) as a\n",
            "\n",
            "Current History State: --- Conversation History ---\n",
            "  [User]: I need to learn about large language models. Where should I start?\n",
            "  [Assistant]: Large language models (LLMs) are a rapidly evolving field in natural language processing (NLP), and there's a lot to learn. Here's a step-by-step guide to help you get started:\n",
            "\n",
            "**1. Understand the basics of NLP**:\n",
            "Begin by learning about the fundamentals of NLP, including:\n",
            "\t* Tokenization\n",
            "\t* Part-of-speech tagging\n",
            "\t* Named entity recognition\n",
            "\t* Sentiment analysis\n",
            "\t* Text classification\n",
            "\n",
            "You can find many online resources\n",
            "  [User]: What's the difference between fine-tuning and retrieval-augmented generation?\n",
            "  [Assistant]: Fine-tuning and retrieval-augmented generation are two popular techniques used in large language models (LLMs) to improve their performance on specific tasks or domains. Here's a brief overview of each technique:\n",
            "\n",
            "**Fine-tuning:**\n",
            "\n",
            "Fine-tuning involves training a pre-trained LLM on a specific dataset or task to adapt it to the target domain. This typically involves:\n",
            "\n",
            "1. Loading a pre-trained LLM (e.g., BERT, RoBERTa, or T5) as a\n",
            "--------------------------\n",
            "\n",
            "--- Turn 3 ---\n",
            "[User]: That makes sense. So RAG is better for knowledge-intensive tasks without retraining?\n",
            "[Assistant]: Retrieval-augmented generation (RAG) is particularly well-suited for knowledge-intensive tasks, such as:\n",
            "\n",
            "1. Question answering (QA)\n",
            "2. Summarization\n",
            "3. Conversational dialogue\n",
            "4. Text generation\n",
            "\n",
            "RAG's strengths lie in its ability to:\n",
            "\n",
            "1. **Retrieve relevant information**: By searching through a large corpus, RAG can access and incorporate relevant knowledge from a wide range of sources.\n",
            "2. **Augment generation**: RAG's retrieval component provides\n",
            "\n",
            ">>> Condition met: Run 3. Triggering summarization...\n",
            ">>> Current history before summarization: --- Conversation History ---\n",
            "  [User]: I need to learn about large language models. Where should I start?\n",
            "  [Assistant]: Large language models (LLMs) are a rapidly evolving field in natural language processing (NLP), and there's a lot to learn. Here's a step-by-step guide to help you get started:\n",
            "\n",
            "**1. Understand the basics of NLP**:\n",
            "Begin by learning about the fundamentals of NLP, including:\n",
            "\t* Tokenization\n",
            "\t* Part-of-speech tagging\n",
            "\t* Named entity recognition\n",
            "\t* Sentiment analysis\n",
            "\t* Text classification\n",
            "\n",
            "You can find many online resources\n",
            "  [User]: What's the difference between fine-tuning and retrieval-augmented generation?\n",
            "  [Assistant]: Fine-tuning and retrieval-augmented generation are two popular techniques used in large language models (LLMs) to improve their performance on specific tasks or domains. Here's a brief overview of each technique:\n",
            "\n",
            "**Fine-tuning:**\n",
            "\n",
            "Fine-tuning involves training a pre-trained LLM on a specific dataset or task to adapt it to the target domain. This typically involves:\n",
            "\n",
            "1. Loading a pre-trained LLM (e.g., BERT, RoBERTa, or T5) as a\n",
            "  [User]: That makes sense. So RAG is better for knowledge-intensive tasks without retraining?\n",
            "  [Assistant]: Retrieval-augmented generation (RAG) is particularly well-suited for knowledge-intensive tasks, such as:\n",
            "\n",
            "1. Question answering (QA)\n",
            "2. Summarization\n",
            "3. Conversational dialogue\n",
            "4. Text generation\n",
            "\n",
            "RAG's strengths lie in its ability to:\n",
            "\n",
            "1. **Retrieve relevant information**: By searching through a large corpus, RAG can access and incorporate relevant knowledge from a wide range of sources.\n",
            "2. **Augment generation**: RAG's retrieval component provides\n",
            "--------------------------\n",
            "\n",
            ">>> Generated Summary: Summary:\n",
            "\n",
            "The conversation revolves around Large Language Models (LLMs) and their related techniques, specifically fine-tuning and retrieval-augmented generation (RAG).\n",
            "\n",
            "**Main Topics:**\n",
            "\n",
            "1. **Getting started with LLMs:**\n",
            "\t* Understanding the basics of NLP (tokenization, POS tagging, NER, sentiment analysis, and text classification)\n",
            "\t* Recommended online resources\n",
            "2. **Fine-tuning:**\n",
            "\t* Definition: adapting a pre-trained LLM to a specific task or domain\n",
            "\t* Steps involved: loading a pre-trained LLM, adding task-specific layers, and training\n",
            "3. **Retrieval-Augmented Generation (RAG):**\n",
            "\t* Definition: combining retrieval and generation techniques for knowledge-intensive tasks\n",
            "\t* Advantages: retrieving relevant information and augmenting generation\n",
            "4. **Comparison of Fine-tuning and RAG:**\n",
            "\t* RAG is better suited for knowledge-intensive tasks (QA, summarization, conversational dialogue, and text generation)\n",
            "\n",
            "**Key Takeaways:**\n",
            "\n",
            "* Fine-tuning is a technique for adapting pre-trained LLMs to specific tasks or domains\n",
            "* RAG is particularly effective for knowledge-intensive tasks that require accessing and incorporating relevant information from a wide range of sources\n",
            "\n",
            ">>> History has been replaced with the summary.\n",
            "\n",
            "Current History State: --- Conversation History ---\n",
            "  [System]: Summary of previous conversation: Summary:\n",
            "\n",
            "The conversation revolves around Large Language Models (LLMs) and their related techniques, specifically fine-tuning and retrieval-augmented generation (RAG).\n",
            "\n",
            "**Main Topics:**\n",
            "\n",
            "1. **Getting started with LLMs:**\n",
            "\t* Understanding the basics of NLP (tokenization, POS tagging, NER, sentiment analysis, and text classification)\n",
            "\t* Recommended online resources\n",
            "2. **Fine-tuning:**\n",
            "\t* Definition: adapting a pre-trained LLM to a specific task or domain\n",
            "\t* Steps involved: loading a pre-trained LLM, adding task-specific layers, and training\n",
            "3. **Retrieval-Augmented Generation (RAG):**\n",
            "\t* Definition: combining retrieval and generation techniques for knowledge-intensive tasks\n",
            "\t* Advantages: retrieving relevant information and augmenting generation\n",
            "4. **Comparison of Fine-tuning and RAG:**\n",
            "\t* RAG is better suited for knowledge-intensive tasks (QA, summarization, conversational dialogue, and text generation)\n",
            "\n",
            "**Key Takeaways:**\n",
            "\n",
            "* Fine-tuning is a technique for adapting pre-trained LLMs to specific tasks or domains\n",
            "* RAG is particularly effective for knowledge-intensive tasks that require accessing and incorporating relevant information from a wide range of sources\n",
            "--------------------------\n",
            "\n",
            "--- Turn 4 ---\n",
            "[User]: Okay, let's switch topics. I want to build a simple Python web app.\n",
            "[Assistant]: Building a simple Python web app can be done using various frameworks such as Flask or Django. For this example, we'll use Flask because it's lightweight and easy to learn.\n",
            "\n",
            "**Step 1: Install Flask**\n",
            "\n",
            "To start, you'll need to install Flask using pip:\n",
            "\n",
            "```bash\n",
            "pip install flask\n",
            "```\n",
            "\n",
            "**Step 2: Create a new Flask app**\n",
            "\n",
            "Now, let's create a new Flask app. Create a new file called `app.py` and add the following code:\n",
            "\n",
            "\n",
            "\n",
            "Current History State: --- Conversation History ---\n",
            "  [System]: Summary of previous conversation: Summary:\n",
            "\n",
            "The conversation revolves around Large Language Models (LLMs) and their related techniques, specifically fine-tuning and retrieval-augmented generation (RAG).\n",
            "\n",
            "**Main Topics:**\n",
            "\n",
            "1. **Getting started with LLMs:**\n",
            "\t* Understanding the basics of NLP (tokenization, POS tagging, NER, sentiment analysis, and text classification)\n",
            "\t* Recommended online resources\n",
            "2. **Fine-tuning:**\n",
            "\t* Definition: adapting a pre-trained LLM to a specific task or domain\n",
            "\t* Steps involved: loading a pre-trained LLM, adding task-specific layers, and training\n",
            "3. **Retrieval-Augmented Generation (RAG):**\n",
            "\t* Definition: combining retrieval and generation techniques for knowledge-intensive tasks\n",
            "\t* Advantages: retrieving relevant information and augmenting generation\n",
            "4. **Comparison of Fine-tuning and RAG:**\n",
            "\t* RAG is better suited for knowledge-intensive tasks (QA, summarization, conversational dialogue, and text generation)\n",
            "\n",
            "**Key Takeaways:**\n",
            "\n",
            "* Fine-tuning is a technique for adapting pre-trained LLMs to specific tasks or domains\n",
            "* RAG is particularly effective for knowledge-intensive tasks that require accessing and incorporating relevant information from a wide range of sources\n",
            "  [User]: Okay, let's switch topics. I want to build a simple Python web app.\n",
            "  [Assistant]: Building a simple Python web app can be done using various frameworks such as Flask or Django. For this example, we'll use Flask because it's lightweight and easy to learn.\n",
            "\n",
            "**Step 1: Install Flask**\n",
            "\n",
            "To start, you'll need to install Flask using pip:\n",
            "\n",
            "```bash\n",
            "pip install flask\n",
            "```\n",
            "\n",
            "**Step 2: Create a new Flask app**\n",
            "\n",
            "Now, let's create a new Flask app. Create a new file called `app.py` and add the following code:\n",
            "\n",
            "\n",
            "--------------------------\n",
            "\n",
            "--- Turn 5 ---\n",
            "[User]: Which framework is easier for a beginner, Flask or Django?\n",
            "[Assistant]: Between Flask and Django, Flask is generally considered easier for a beginner to learn and use, especially for small to medium-sized projects.\n",
            "\n",
            "Here's why:\n",
            "\n",
            "1. **Lightweight**: Flask is a microframework, meaning it has a smaller codebase and fewer dependencies than Django. This makes it easier to understand and learn.\n",
            "2. **Minimal overhead**: Flask doesn't come with a lot of built-in features, which can be overwhelming for beginners. Instead, it provides a simple and flexible way to build\n",
            "\n",
            "Current History State: --- Conversation History ---\n",
            "  [System]: Summary of previous conversation: Summary:\n",
            "\n",
            "The conversation revolves around Large Language Models (LLMs) and their related techniques, specifically fine-tuning and retrieval-augmented generation (RAG).\n",
            "\n",
            "**Main Topics:**\n",
            "\n",
            "1. **Getting started with LLMs:**\n",
            "\t* Understanding the basics of NLP (tokenization, POS tagging, NER, sentiment analysis, and text classification)\n",
            "\t* Recommended online resources\n",
            "2. **Fine-tuning:**\n",
            "\t* Definition: adapting a pre-trained LLM to a specific task or domain\n",
            "\t* Steps involved: loading a pre-trained LLM, adding task-specific layers, and training\n",
            "3. **Retrieval-Augmented Generation (RAG):**\n",
            "\t* Definition: combining retrieval and generation techniques for knowledge-intensive tasks\n",
            "\t* Advantages: retrieving relevant information and augmenting generation\n",
            "4. **Comparison of Fine-tuning and RAG:**\n",
            "\t* RAG is better suited for knowledge-intensive tasks (QA, summarization, conversational dialogue, and text generation)\n",
            "\n",
            "**Key Takeaways:**\n",
            "\n",
            "* Fine-tuning is a technique for adapting pre-trained LLMs to specific tasks or domains\n",
            "* RAG is particularly effective for knowledge-intensive tasks that require accessing and incorporating relevant information from a wide range of sources\n",
            "  [User]: Okay, let's switch topics. I want to build a simple Python web app.\n",
            "  [Assistant]: Building a simple Python web app can be done using various frameworks such as Flask or Django. For this example, we'll use Flask because it's lightweight and easy to learn.\n",
            "\n",
            "**Step 1: Install Flask**\n",
            "\n",
            "To start, you'll need to install Flask using pip:\n",
            "\n",
            "```bash\n",
            "pip install flask\n",
            "```\n",
            "\n",
            "**Step 2: Create a new Flask app**\n",
            "\n",
            "Now, let's create a new Flask app. Create a new file called `app.py` and add the following code:\n",
            "\n",
            "\n",
            "  [User]: Which framework is easier for a beginner, Flask or Django?\n",
            "  [Assistant]: Between Flask and Django, Flask is generally considered easier for a beginner to learn and use, especially for small to medium-sized projects.\n",
            "\n",
            "Here's why:\n",
            "\n",
            "1. **Lightweight**: Flask is a microframework, meaning it has a smaller codebase and fewer dependencies than Django. This makes it easier to understand and learn.\n",
            "2. **Minimal overhead**: Flask doesn't come with a lot of built-in features, which can be overwhelming for beginners. Instead, it provides a simple and flexible way to build\n",
            "--------------------------\n",
            "\n",
            "--- Turn 6 ---\n",
            "[User]: Thanks. I'll start with Flask. Can you give me a 'Hello, World!' example?\n",
            "[Assistant]: Here's a simple \"Hello, World!\" example using Flask:\n",
            "\n",
            "```python\n",
            "# app.py\n",
            "\n",
            "from flask import Flask\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "@app.route(\"/\")\n",
            "def hello_world():\n",
            "    return \"Hello, World!\"\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    app.run(debug=True)\n",
            "```\n",
            "\n",
            "Let me explain what this code does:\n",
            "\n",
            "1. We import the `Flask` class from the `flask` module.\n",
            "2. We create a new Flask app instance, passing the current\n",
            "\n",
            ">>> Condition met: Run 6. Triggering summarization...\n",
            ">>> Current history before summarization: --- Conversation History ---\n",
            "  [System]: Summary of previous conversation: Summary:\n",
            "\n",
            "The conversation revolves around Large Language Models (LLMs) and their related techniques, specifically fine-tuning and retrieval-augmented generation (RAG).\n",
            "\n",
            "**Main Topics:**\n",
            "\n",
            "1. **Getting started with LLMs:**\n",
            "\t* Understanding the basics of NLP (tokenization, POS tagging, NER, sentiment analysis, and text classification)\n",
            "\t* Recommended online resources\n",
            "2. **Fine-tuning:**\n",
            "\t* Definition: adapting a pre-trained LLM to a specific task or domain\n",
            "\t* Steps involved: loading a pre-trained LLM, adding task-specific layers, and training\n",
            "3. **Retrieval-Augmented Generation (RAG):**\n",
            "\t* Definition: combining retrieval and generation techniques for knowledge-intensive tasks\n",
            "\t* Advantages: retrieving relevant information and augmenting generation\n",
            "4. **Comparison of Fine-tuning and RAG:**\n",
            "\t* RAG is better suited for knowledge-intensive tasks (QA, summarization, conversational dialogue, and text generation)\n",
            "\n",
            "**Key Takeaways:**\n",
            "\n",
            "* Fine-tuning is a technique for adapting pre-trained LLMs to specific tasks or domains\n",
            "* RAG is particularly effective for knowledge-intensive tasks that require accessing and incorporating relevant information from a wide range of sources\n",
            "  [User]: Okay, let's switch topics. I want to build a simple Python web app.\n",
            "  [Assistant]: Building a simple Python web app can be done using various frameworks such as Flask or Django. For this example, we'll use Flask because it's lightweight and easy to learn.\n",
            "\n",
            "**Step 1: Install Flask**\n",
            "\n",
            "To start, you'll need to install Flask using pip:\n",
            "\n",
            "```bash\n",
            "pip install flask\n",
            "```\n",
            "\n",
            "**Step 2: Create a new Flask app**\n",
            "\n",
            "Now, let's create a new Flask app. Create a new file called `app.py` and add the following code:\n",
            "\n",
            "\n",
            "  [User]: Which framework is easier for a beginner, Flask or Django?\n",
            "  [Assistant]: Between Flask and Django, Flask is generally considered easier for a beginner to learn and use, especially for small to medium-sized projects.\n",
            "\n",
            "Here's why:\n",
            "\n",
            "1. **Lightweight**: Flask is a microframework, meaning it has a smaller codebase and fewer dependencies than Django. This makes it easier to understand and learn.\n",
            "2. **Minimal overhead**: Flask doesn't come with a lot of built-in features, which can be overwhelming for beginners. Instead, it provides a simple and flexible way to build\n",
            "  [User]: Thanks. I'll start with Flask. Can you give me a 'Hello, World!' example?\n",
            "  [Assistant]: Here's a simple \"Hello, World!\" example using Flask:\n",
            "\n",
            "```python\n",
            "# app.py\n",
            "\n",
            "from flask import Flask\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "@app.route(\"/\")\n",
            "def hello_world():\n",
            "    return \"Hello, World!\"\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    app.run(debug=True)\n",
            "```\n",
            "\n",
            "Let me explain what this code does:\n",
            "\n",
            "1. We import the `Flask` class from the `flask` module.\n",
            "2. We create a new Flask app instance, passing the current\n",
            "--------------------------\n",
            "\n",
            ">>> Generated Summary: Summary of Conversation:\n",
            "\n",
            "**Main Topics:**\n",
            "\n",
            "1. **Python Web Development:**\n",
            "\t* Introduction to Flask, a lightweight and easy-to-learn web framework\n",
            "\t* Comparison between Flask and Django for beginner-friendly frameworks\n",
            "2. **Getting Started with Flask:**\n",
            "\t* Installing Flask using pip\n",
            "\t* Creating a new Flask app\n",
            "3. **'Hello, World!' Example:**\n",
            "\t* A simple example code to demonstrate the basic setup of a Flask app\n",
            "\n",
            "**Key Takeaways:**\n",
            "\n",
            "* Flask is a beginner-friendly framework for building web applications\n",
            "* It is easier to learn and use than Django, especially for small to medium-sized projects\n",
            "* Flask provides a lightweight and flexible way to build web applications\n",
            "\n",
            "**Key Points and Decisions:**\n",
            "\n",
            "* The user decided to start with Flask due to its ease of use and lightweight nature\n",
            "* A simple \"Hello, World!\" example code was provided to demonstrate the basic setup of a Flask app\n",
            "\n",
            ">>> History has been replaced with the summary.\n",
            "\n",
            "Current History State: --- Conversation History ---\n",
            "  [System]: Summary of previous conversation: Summary of Conversation:\n",
            "\n",
            "**Main Topics:**\n",
            "\n",
            "1. **Python Web Development:**\n",
            "\t* Introduction to Flask, a lightweight and easy-to-learn web framework\n",
            "\t* Comparison between Flask and Django for beginner-friendly frameworks\n",
            "2. **Getting Started with Flask:**\n",
            "\t* Installing Flask using pip\n",
            "\t* Creating a new Flask app\n",
            "3. **'Hello, World!' Example:**\n",
            "\t* A simple example code to demonstrate the basic setup of a Flask app\n",
            "\n",
            "**Key Takeaways:**\n",
            "\n",
            "* Flask is a beginner-friendly framework for building web applications\n",
            "* It is easier to learn and use than Django, especially for small to medium-sized projects\n",
            "* Flask provides a lightweight and flexible way to build web applications\n",
            "\n",
            "**Key Points and Decisions:**\n",
            "\n",
            "* The user decided to start with Flask due to its ease of use and lightweight nature\n",
            "* A simple \"Hello, World!\" example code was provided to demonstrate the basic setup of a Flask app\n",
            "--------------------------\n",
            "\n",
            "======================== TASK 2 DEMONSTRATION ========================\n",
            "\n",
            "--- Processing Sample 1 ---\n",
            "Input Chat: \"Hi, I'm John Doe and I'm 29. My email is john.doe@example.com and you can reach me at (123) 456-7890. I'm currently in New York.\"\n",
            "\n",
            "üîç Extracted Information: {\n",
            "  \"age\": 29,\n",
            "  \"email\": \"john.doe@example.com\",\n",
            "  \"location\": \"New York\",\n",
            "  \"name\": \"John Doe\",\n",
            "  \"phone\": \"(123) 456-7890\"\n",
            "}\n",
            "\n",
            "Validation Result: ‚úÖ Validation Passed: Extracted data conforms to the schema.\n",
            "\n",
            "--- Processing Sample 2 ---\n",
            "Input Chat: \"Please sign up Jane Smith for the newsletter at jane.s@email.net.\"\n",
            "\n",
            "üîç Extracted Information: {\n",
            "  \"email\": \"jane.s@email.net\",\n",
            "  \"name\": \"Jane Smith\"\n",
            "}\n",
            "\n",
            "Validation Result: ‚úÖ Validation Passed: Extracted data conforms to the schema.\n",
            "\n",
            "--- Processing Sample 3 ---\n",
            "Input Chat: \"Hello, I'd like to know more about your services. What do you offer?\"\n",
            "\n",
            "‚úÖ Result: No structured information was extracted, as expected for this input.\n",
            "\n",
            "--- Processing Sample 4 ---\n",
            "Input Chat: \"My name is Priya Sharma, I live in Mumbai. I am 35 years old. My phone is +91 98765 43210.\"\n",
            "\n",
            "üîç Extracted Information: {\n",
            "  \"age\": 35,\n",
            "  \"location\": \"Mumbai\",\n",
            "  \"name\": \"Priya Sharma\",\n",
            "  \"phone\": \"+91 98765 43210\"\n",
            "}\n",
            "\n",
            "Validation Result: ‚úÖ Validation Passed: Extracted data conforms to the schema.\n",
            "\n",
            "======================================================================\n",
            "‚úÖ All Demonstrations Finished.\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "# @title 1. Setup, Configuration, and Dependencies\n",
        "# Install the necessary Python library for Groq\n",
        "!pip install groq\n",
        "\n",
        "import os\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "import json\n",
        "\n",
        "print(\"‚úÖ Setup Complete: Libraries installed and imported.\")\n",
        "\n",
        "# --- API Key Configuration ---\n",
        "try:\n",
        "    groq_api_key = userdata.get('GROQ_API_KEY')\n",
        "    os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
        "    print(\"‚úÖ Groq API Key loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"üö® Error loading Groq API Key. Please set it in Colab's Secrets Manager.\")\n",
        "    groq_api_key = None\n",
        "\n",
        "# --- Centralized Model Configuration ---\n",
        "# üí° If you get a 'model decommissioned' error, find a new model name from\n",
        "# https://console.groq.com/docs/models and update the variable below.\n",
        "ACTIVE_MODEL = \"llama-3.1-8b-instant\"\n",
        "\n",
        "# --- Initialize Groq Client ---\n",
        "if groq_api_key:\n",
        "    try:\n",
        "        client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "        print(f\"‚úÖ Groq client initialized. Using model: {ACTIVE_MODEL}\")\n",
        "    except Exception as e:\n",
        "        client = None\n",
        "        print(f\"üö® Failed to initialize Groq client: {e}\")\n",
        "else:\n",
        "    client = None\n",
        "    print(\"üö® Groq client not initialized because API key is missing.\")\n",
        "\n",
        "\n",
        "# %%\n",
        "# @title 2. Helper Functions (for both tasks)\n",
        "\n",
        "def format_history_for_display(history):\n",
        "    \"\"\"Helper to pretty-print the conversation history.\"\"\"\n",
        "    formatted_string = \"--- Conversation History ---\\n\"\n",
        "    if not history: return formatted_string + \"  [Empty]\\n\"\n",
        "    for msg in history: formatted_string += f\"  [{msg['role'].capitalize()}]: {msg['content']}\\n\"\n",
        "    formatted_string += \"--------------------------\"\n",
        "    return formatted_string\n",
        "\n",
        "# --- Task 1 Helper Functions ---\n",
        "\n",
        "def get_assistant_response(messages):\n",
        "    \"\"\"Gets a conversational response from the Groq API.\"\"\"\n",
        "    if not client: return \"Groq client not initialized.\"\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=messages,\n",
        "            model=ACTIVE_MODEL, # Uses the central model variable\n",
        "            temperature=0.7,\n",
        "            max_tokens=100,\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "    except Exception as e: return f\"Error communicating with Groq API: {e}\"\n",
        "\n",
        "def truncate_by_turns(history, n_turns):\n",
        "    \"\"\"Limits the history to the last n conversation turns.\"\"\"\n",
        "    return history[-(n_turns * 2):]\n",
        "\n",
        "def truncate_by_length(history, max_chars):\n",
        "    \"\"\"Limits history by character count, starting from the most recent message.\"\"\"\n",
        "    truncated_history = []\n",
        "    current_chars = 0\n",
        "    for message in reversed(history):\n",
        "        message_chars = len(message.get('content', ''))\n",
        "        if current_chars + message_chars <= max_chars:\n",
        "            truncated_history.insert(0, message)\n",
        "            current_chars += message_chars\n",
        "        else: break\n",
        "    return truncated_history\n",
        "\n",
        "def summarize_conversation(history):\n",
        "    \"\"\"Uses the Groq API to summarize the conversation history.\"\"\"\n",
        "    if not client: return \"Groq client not initialized.\"\n",
        "    conversation_text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in history])\n",
        "    summarization_prompt = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert summarizer. Create a concise, neutral summary of the following chat. Capture the main topics, questions, and conclusions.\"},\n",
        "        {\"role\": \"user\", \"content\": conversation_text}\n",
        "    ]\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=summarization_prompt,\n",
        "            model=ACTIVE_MODEL, # Uses the central model variable\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "    except Exception as e: return f\"Error during summarization: {e}\"\n",
        "\n",
        "# --- Task 2 Helper Functions ---\n",
        "\n",
        "USER_DETAILS_TOOL = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"extract_user_info\",\n",
        "        \"description\": \"Extracts user details from the text. Only include fields for which a value is explicitly provided. Do not include fields with null values.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"The full name of the user.\"},\n",
        "                \"email\": {\"type\": \"string\", \"description\": \"The user's email address.\"},\n",
        "                \"phone\": {\"type\": \"string\", \"description\": \"The user's phone number.\"},\n",
        "                \"location\": {\"type\": \"string\", \"description\": \"The user's city, state, or country.\"},\n",
        "                \"age\": {\"type\": \"integer\", \"description\": \"The age of the user.\"},\n",
        "            },\n",
        "            \"required\": [],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "def extract_info_from_chat(chat_message):\n",
        "    \"\"\"Uses the Groq API with tool calling to extract structured data.\"\"\"\n",
        "    if not client: return None\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=ACTIVE_MODEL, # Uses the central model variable\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Your ONLY task is to use the provided `extract_user_info` tool to capture relevant information. Do not infer other tool names. Focus only on extracting details.\"},\n",
        "                {\"role\": \"user\", \"content\": chat_message}\n",
        "            ],\n",
        "            tools=[USER_DETAILS_TOOL],\n",
        "            tool_choice=\"auto\",\n",
        "        )\n",
        "        tool_call = response.choices[0].message.tool_calls\n",
        "        if tool_call: return json.loads(tool_call[0].function.arguments)\n",
        "        else: return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during API call: {e}\")\n",
        "        return None\n",
        "\n",
        "def validate_extracted_data(data, schema):\n",
        "    \"\"\"Performs a simple, manual validation of the extracted data.\"\"\"\n",
        "    if not isinstance(data, dict): return False, \"Validation Failed: Not a dictionary.\"\n",
        "    allowed_properties = schema['function']['parameters']['properties']\n",
        "    for key, value in data.items():\n",
        "        if key not in allowed_properties: return False, f\"Validation Failed: Unexpected field '{key}'.\"\n",
        "        expected_type_str = allowed_properties[key]['type']\n",
        "        expected_type = {'string': str, 'integer': int}.get(expected_type_str)\n",
        "        if not isinstance(value, expected_type): return False, f\"Validation Failed: Field '{key}' has wrong type.\"\n",
        "    return True, \"‚úÖ Validation Passed: Extracted data conforms to the schema.\"\n",
        "\n",
        "\n",
        "# %%\n",
        "# @title 3. Task 1: Conversation History and Summarization\n",
        "\n",
        "class ConversationManager:\n",
        "    \"\"\"Manages the conversation history, including periodic summarization.\"\"\"\n",
        "    def __init__(self, k_runs_for_summarization=3):\n",
        "        self.history = []\n",
        "        self.run_counter = 0\n",
        "        self.k = k_runs_for_summarization\n",
        "        print(f\"ConversationManager initialized. Summarization will occur after every {self.k} turns.\")\n",
        "\n",
        "    def add_turn(self, user_input):\n",
        "        print(f\"\\n--- Turn {self.run_counter + 1} ---\")\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
        "        assistant_response = get_assistant_response(self.history)\n",
        "        self.history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "        print(f\"[User]: {user_input}\\n[Assistant]: {assistant_response}\")\n",
        "        self.run_counter += 1\n",
        "        if self.run_counter % self.k == 0:\n",
        "            print(f\"\\n>>> Condition met: Run {self.run_counter}. Triggering summarization...\")\n",
        "            self.summarize_and_replace()\n",
        "\n",
        "    def summarize_and_replace(self):\n",
        "        print(\">>> Current history before summarization:\", format_history_for_display(self.history))\n",
        "        summary = summarize_conversation(self.history)\n",
        "        print(\"\\n>>> Generated Summary:\", summary)\n",
        "        self.history = [{\"role\": \"system\", \"content\": f\"Summary of previous conversation: {summary}\"}]\n",
        "        print(\"\\n>>> History has been replaced with the summary.\")\n",
        "\n",
        "# --- DEMONSTRATION FOR TASK 1 ---\n",
        "print(\" TASK 1 DEMONSTRATION \".center(70, \"=\"))\n",
        "conversation_flow = [\n",
        "    \"I need to learn about large language models. Where should I start?\",\n",
        "    \"What's the difference between fine-tuning and retrieval-augmented generation?\",\n",
        "    \"That makes sense. So RAG is better for knowledge-intensive tasks without retraining?\",\n",
        "    \"Okay, let's switch topics. I want to build a simple Python web app.\",\n",
        "    \"Which framework is easier for a beginner, Flask or Django?\",\n",
        "    \"Thanks. I'll start with Flask. Can you give me a 'Hello, World!' example?\",\n",
        "]\n",
        "manager = ConversationManager(k_runs_for_summarization=3)\n",
        "if client:\n",
        "    for user_prompt in conversation_flow:\n",
        "        manager.add_turn(user_prompt)\n",
        "        print(\"\\nCurrent History State:\", format_history_for_display(manager.history))\n",
        "else:\n",
        "    print(\"\\nüö® Halting Task 1 demonstration because Groq client is not initialized.\")\n",
        "\n",
        "# %%\n",
        "# @title 4. Task 2: JSON Schema Classification & Information Extraction\n",
        "\n",
        "# --- DEMONSTRATION FOR TASK 2 ---\n",
        "print(\"\\n\" + \" TASK 2 DEMONSTRATION \".center(70, \"=\"))\n",
        "sample_chats = [\n",
        "    \"Hi, I'm John Doe and I'm 29. My email is john.doe@example.com and you can reach me at (123) 456-7890. I'm currently in New York.\",\n",
        "    \"Please sign up Jane Smith for the newsletter at jane.s@email.net.\",\n",
        "    \"Hello, I'd like to know more about your services. What do you offer?\",\n",
        "    \"My name is Priya Sharma, I live in Mumbai. I am 35 years old. My phone is +91 98765 43210.\"\n",
        "]\n",
        "\n",
        "if client:\n",
        "    for i, chat in enumerate(sample_chats):\n",
        "        print(f\"\\n--- Processing Sample {i+1} ---\")\n",
        "        print(f\"Input Chat: \\\"{chat}\\\"\")\n",
        "        extracted_data = extract_info_from_chat(chat)\n",
        "        if extracted_data:\n",
        "            print(\"\\nüîç Extracted Information:\", json.dumps(extracted_data, indent=2))\n",
        "            is_valid, message = validate_extracted_data(extracted_data, USER_DETAILS_TOOL)\n",
        "            print(f\"\\nValidation Result: {message}\")\n",
        "        else:\n",
        "            print(\"\\n‚úÖ Result: No structured information was extracted, as expected for this input.\")\n",
        "else:\n",
        "    print(\"\\nüö® Halting Task 2 demonstration because Groq client is not initialized.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ All Demonstrations Finished.\")"
      ]
    }
  ]
}